âš– 2. Feature Weighting
ğŸ”¹ What it is:

Before clustering, we assign different weights to each feature.

Example:

weights = {'Recency': 1.5, 'Frequency': 1.0, 'Monetary': 2.0}

ğŸ”¹ Why we use it:

Not all behaviors are equally important:

Spending more money (Monetary) might matter more than purchase frequency.

Recency might influence re-targeting more than raw frequency.

So, we use feature weighting to guide K-Means â€”
clusters will form giving more â€œpullâ€ to high-weight features.

ğŸ”¹ How it affects K-Means:

Normally, K-Means calculates Euclidean distance as:

ğ·
=
(
ğ‘Ÿ
1
âˆ’
ğ‘Ÿ
2
)
2
+
(
ğ‘“
1
âˆ’
ğ‘“
2
)
2
+
(
ğ‘š
1
âˆ’
ğ‘š
2
)
2
D=
(r
1
	â€‹

âˆ’r
2
	â€‹

)
2
+(f
1
	â€‹

âˆ’f
2
	â€‹

)
2
+(m
1
	â€‹

âˆ’m
2
	â€‹

)
2
	â€‹


After weighting:

ğ·
=
ğ‘¤
ğ‘Ÿ
(
ğ‘Ÿ
1
âˆ’
ğ‘Ÿ
2
)
2
+
ğ‘¤
ğ‘“
(
ğ‘“
1
âˆ’
ğ‘“
2
)
2
+
ğ‘¤
ğ‘š
(
ğ‘š
1
âˆ’
ğ‘š
2
)
2
D=
w
r
	â€‹

(r
1
	â€‹

âˆ’r
2
	â€‹

)
2
+w
f
	â€‹

(f
1
	â€‹

âˆ’f
2
	â€‹

)
2
+w
m
	â€‹

(m
1
	â€‹

âˆ’m
2
	â€‹

)
2
	â€‹


So, a large 
ğ‘¤
ğ‘š
w
m
	â€‹

 makes monetary difference dominate distance â€”
helping form clusters based on spending patterns.

ğŸ§® 3. Standardization (Feature Scaling)
ğŸ”¹ What it is:

Transform features to a similar scale â€” usually mean=0 and std=1.

ğŸ”¹ Why we use it:

Because R, F, and M are measured in different units:

Recency â†’ days (0â€“400+)

Frequency â†’ counts (1â€“100+)

Monetary â†’ dollars or pounds (up to thousands)

Without scaling, large-scale features (like Monetary) would dominate the clustering unfairly â€” even more than weights intend.

ğŸ”¹ How itâ€™s done:
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_weighted)

ğŸ”¹ 4. Finding the Optimal K (Elbow + Silhouette)
ğŸ”¹ What it is:

K-Means needs a predefined number of clusters (k).
We use two methods to find a good k automatically.

a. Elbow Method:

Plots within-cluster sum of squares (WCSS) vs. k

Look for the â€œelbowâ€ point where reduction slows down.

b. Silhouette Score:

Measures how well each point fits its own cluster vs. others

Higher score = better separation

ğŸ”¹ Why we use it:

So we donâ€™t choose k arbitrarily â€” itâ€™s data-driven.

ğŸ”¹ How it contributes:

Your algorithm tests multiple k values (say 2â€“10), computes both scores, and then picks:

best_k = silhouette_scores.index(max(silhouette_scores)) + 2


âœ… Ensures best-performing k for your specific dataset.

ğŸŒ€ 5. Weighted K-Means Clustering
ğŸ”¹ What it is:

We run standard K-Means, but on the weighted and scaled RFM features.

ğŸ”¹ Why we use it:

K-Means minimizes the sum of squared distances between points and their cluster centroid â€” so after weighting, it effectively finds groups emphasizing chosen attributes.

ğŸ”¹ What happens internally:

Randomly pick k centroids

Assign each point to nearest centroid (based on weighted Euclidean distance)

Recalculate centroids (weighted mean)

Repeat until convergence

ğŸ”¹ How it contributes:

Groups customers into behavioral clusters â€” e.g.:

High spending, frequent buyers

Rare buyers, low spenders

Recently active but not high spenders

ğŸ· 6. Cluster Labeling (Customer Type Naming)
ğŸ”¹ What it is:

After clustering, compute cluster averages:

cluster_summary = rfm.groupby('Cluster')[['Recency','Frequency','Monetary']].mean()


Then name clusters based on relative behavior (auto-generated logic):

if low recency + high freq + high monetary â†’ â€œChampionsâ€
if low recency + high freq â†’ â€œLoyalâ€
if high recency + low freq â†’ â€œLostâ€
else â†’ â€œRegularâ€

ğŸ”¹ Why we use it:

This gives business meaning to numeric clusters â€” making the results interpretable.

ğŸ”¹ How it contributes:

It translates numeric clusters into behavioral personas,
